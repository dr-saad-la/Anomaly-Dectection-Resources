{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c05db5-b40c-4367-81f7-298250d52729",
   "metadata": {},
   "source": [
    "# Anomaly Detection Benchmark Data\n",
    "\n",
    "In this notebook, we will demonstrate how to download anomaly detection benchmark datasets. These datasets are used for evaluating and testing various **anomaly detection algorithms**.\n",
    "\n",
    "We will explore two methods to obtain the datasets:\n",
    "\n",
    "1. Using the `adbench` library to download data.\n",
    "2. Cloning the repository directly.\n",
    "\n",
    "\n",
    "## Method 1: Using the `adbench` Library\n",
    "\n",
    "1. **Check installation**: Make sure the `adbench` library is installed. If it's not installed, please refer to [anomaly detectin environment setup](00-setup.ipynb) for detailed installation instructions.\n",
    "2. **Download Datasets Using adbench**: Use the following Python script to download the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0c7f36-4cf3-47eb-9be4-43c1edbafe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if there is any question while downloading datasets, we suggest you to download it from the website:\n",
      "https://github.com/Minqi824/ADBench/tree/main/adbench/datasets\n",
      "如果您在中国大陆地区，请使用链接：\n",
      "https://jihulab.com/BraudoCC/ADBench_datasets/\n",
      "Downloading datasets from jihulab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1265.89it/s]\n",
      "INFO:__main__:Datasets downloaded successfully using adbench.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10_0.npz already exists. Skipping download...\n",
      "CIFAR10_1.npz already exists. Skipping download...\n",
      "CIFAR10_2.npz already exists. Skipping download...\n",
      "CIFAR10_3.npz already exists. Skipping download...\n",
      "CIFAR10_4.npz already exists. Skipping download...\n",
      "CIFAR10_5.npz already exists. Skipping download...\n",
      "CIFAR10_6.npz already exists. Skipping download...\n",
      "CIFAR10_7.npz already exists. Skipping download...\n",
      "CIFAR10_8.npz already exists. Skipping download...\n",
      "CIFAR10_9.npz already exists. Skipping download...\n",
      "FashionMNIST_0.npz already exists. Skipping download...\n",
      "FashionMNIST_1.npz already exists. Skipping download...\n",
      "FashionMNIST_2.npz already exists. Skipping download...\n",
      "FashionMNIST_3.npz already exists. Skipping download...\n",
      "FashionMNIST_4.npz already exists. Skipping download...\n",
      "FashionMNIST_5.npz already exists. Skipping download...\n",
      "FashionMNIST_6.npz already exists. Skipping download...\n",
      "FashionMNIST_7.npz already exists. Skipping download...\n",
      "FashionMNIST_8.npz already exists. Skipping download...\n",
      "FashionMNIST_9.npz already exists. Skipping download...\n",
      "MNIST-C_brightness.npz already exists. Skipping download...\n",
      "MNIST-C_canny_edges.npz already exists. Skipping download...\n",
      "MNIST-C_dotted_line.npz already exists. Skipping download...\n",
      "MNIST-C_fog.npz already exists. Skipping download...\n",
      "MNIST-C_glass_blur.npz already exists. Skipping download...\n",
      "MNIST-C_identity.npz already exists. Skipping download...\n",
      "MNIST-C_impulse_noise.npz already exists. Skipping download...\n",
      "MNIST-C_motion_blur.npz already exists. Skipping download...\n",
      "MNIST-C_rotate.npz already exists. Skipping download...\n",
      "MNIST-C_scale.npz already exists. Skipping download...\n",
      "MNIST-C_shear.npz already exists. Skipping download...\n",
      "MNIST-C_shot_noise.npz already exists. Skipping download...\n",
      "MNIST-C_spatter.npz already exists. Skipping download...\n",
      "MNIST-C_stripe.npz already exists. Skipping download...\n",
      "MNIST-C_translate.npz already exists. Skipping download...\n",
      "MNIST-C_zigzag.npz already exists. Skipping download...\n",
      "MVTec-AD_bottle.npz already exists. Skipping download...\n",
      "MVTec-AD_cable.npz already exists. Skipping download...\n",
      "MVTec-AD_capsule.npz already exists. Skipping download...\n",
      "MVTec-AD_carpet.npz already exists. Skipping download...\n",
      "MVTec-AD_grid.npz already exists. Skipping download...\n",
      "MVTec-AD_hazelnut.npz already exists. Skipping download...\n",
      "MVTec-AD_leather.npz already exists. Skipping download...\n",
      "MVTec-AD_metal_nut.npz already exists. Skipping download...\n",
      "MVTec-AD_pill.npz already exists. Skipping download...\n",
      "MVTec-AD_screw.npz already exists. Skipping download...\n",
      "MVTec-AD_tile.npz already exists. Skipping download...\n",
      "MVTec-AD_toothbrush.npz already exists. Skipping download...\n",
      "MVTec-AD_transistor.npz already exists. Skipping download...\n",
      "MVTec-AD_wood.npz already exists. Skipping download...\n",
      "MVTec-AD_zipper.npz already exists. Skipping download...\n",
      "SVHN_0.npz already exists. Skipping download...\n",
      "SVHN_1.npz already exists. Skipping download...\n",
      "SVHN_2.npz already exists. Skipping download...\n",
      "SVHN_3.npz already exists. Skipping download...\n",
      "SVHN_4.npz already exists. Skipping download...\n",
      "SVHN_5.npz already exists. Skipping download...\n",
      "SVHN_6.npz already exists. Skipping download...\n",
      "SVHN_7.npz already exists. Skipping download...\n",
      "SVHN_8.npz already exists. Skipping download...\n",
      "SVHN_9.npz already exists. Skipping download...\n",
      "20news_0.npz already exists. Skipping download...\n",
      "20news_1.npz already exists. Skipping download...\n",
      "20news_2.npz already exists. Skipping download...\n",
      "20news_3.npz already exists. Skipping download...\n",
      "20news_4.npz already exists. Skipping download...\n",
      "20news_5.npz already exists. Skipping download...\n",
      "agnews_0.npz already exists. Skipping download...\n",
      "agnews_1.npz already exists. Skipping download...\n",
      "agnews_2.npz already exists. Skipping download...\n",
      "agnews_3.npz already exists. Skipping download...\n",
      "amazon.npz already exists. Skipping download...\n",
      "imdb.npz already exists. Skipping download...\n",
      "yelp.npz already exists. Skipping download...\n",
      "10_cover.npz already exists. Skipping download...\n",
      "11_donors.npz already exists. Skipping download...\n",
      "12_fault.npz already exists. Skipping download...\n",
      "13_fraud.npz already exists. Skipping download...\n",
      "14_glass.npz already exists. Skipping download...\n",
      "15_Hepatitis.npz already exists. Skipping download...\n",
      "16_http.npz already exists. Skipping download...\n",
      "17_InternetAds.npz already exists. Skipping download...\n",
      "18_Ionosphere.npz already exists. Skipping download...\n",
      "19_landsat.npz already exists. Skipping download...\n",
      "1_ALOI.npz already exists. Skipping download...\n",
      "20_letter.npz already exists. Skipping download...\n",
      "21_Lymphography.npz already exists. Skipping download...\n",
      "22_magic.gamma.npz already exists. Skipping download...\n",
      "23_mammography.npz already exists. Skipping download...\n",
      "24_mnist.npz already exists. Skipping download...\n",
      "25_musk.npz already exists. Skipping download...\n",
      "26_optdigits.npz already exists. Skipping download...\n",
      "27_PageBlocks.npz already exists. Skipping download...\n",
      "28_pendigits.npz already exists. Skipping download...\n",
      "29_Pima.npz already exists. Skipping download...\n",
      "2_annthyroid.npz already exists. Skipping download...\n",
      "30_satellite.npz already exists. Skipping download...\n",
      "31_satimage-2.npz already exists. Skipping download...\n",
      "32_shuttle.npz already exists. Skipping download...\n",
      "33_skin.npz already exists. Skipping download...\n",
      "34_smtp.npz already exists. Skipping download...\n",
      "35_SpamBase.npz already exists. Skipping download...\n",
      "36_speech.npz already exists. Skipping download...\n",
      "37_Stamps.npz already exists. Skipping download...\n",
      "38_thyroid.npz already exists. Skipping download...\n",
      "39_vertebral.npz already exists. Skipping download...\n",
      "3_backdoor.npz already exists. Skipping download...\n",
      "40_vowels.npz already exists. Skipping download...\n",
      "41_Waveform.npz already exists. Skipping download...\n",
      "42_WBC.npz already exists. Skipping download...\n",
      "43_WDBC.npz already exists. Skipping download...\n",
      "44_Wilt.npz already exists. Skipping download...\n",
      "45_wine.npz already exists. Skipping download...\n",
      "46_WPBC.npz already exists. Skipping download...\n",
      "47_yeast.npz already exists. Skipping download...\n",
      "4_breastw.npz already exists. Skipping download...\n",
      "5_campaign.npz already exists. Skipping download...\n",
      "6_cardio.npz already exists. Skipping download...\n",
      "7_Cardiotocography.npz already exists. Skipping download...\n",
      "8_celeba.npz already exists. Skipping download...\n",
      "9_census.npz already exists. Skipping download...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from adbench.myutils import Utils\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the utility class\n",
    "utils = Utils()\n",
    "\n",
    "# Attempt to download datasets with error handling\n",
    "try:\n",
    "    # Download datasets from the specified repository\n",
    "    utils.download_datasets(repo='jihulab')                          # Use 'jihulab' if you're in China\n",
    "    logger.info(\"Datasets downloaded successfully using adbench.\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        logger.info(\"Datasets are already downloaded.\")\n",
    "    else:\n",
    "        # Log unexpected exceptions\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27cc2e-f6be-456a-88c6-eaf83e73f42b",
   "metadata": {},
   "source": [
    "After running the previous script, I had to go through the source code of `download_datasets()` method to figure out the directory where the data was stored. \n",
    "\n",
    "I also did the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf0666c-0c3e-42f9-9ee6-6dc0d7e5f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "# 1. import datasets from adbench\n",
    "from adbench import datasets\n",
    "\n",
    "# 2. Check attributes of this module\n",
    "print(dir(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5b5fa0-8d34-4f05-bc51-780b0669f0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/homebrew/Caskroom/mambaforge/base/envs/anom-detect-env/lib/python3.9/site-packages/adbench/datasets']\n",
      "['CV_by_ResNet18', '__init__.py', '__pycache__', 'Classical', 'data_generator.py', 'NLP_by_BERT']\n"
     ]
    }
   ],
   "source": [
    "# 3. Cheching the __path__ attribute\n",
    "print(datasets.__path__)\n",
    "\n",
    "# 4. List the content of directory\n",
    "import os\n",
    "print(os.listdir(datasets.__path__[0]))\n",
    "\n",
    "# Path to the datasets directory\n",
    "datasets_path = datasets.__path__[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a2384a-862e-419e-bbc4-2739335d845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV_by_ResNet18 directory:\n",
      " 1 : MNIST-C_spatter.npz\n",
      " 2 : MNIST-C_glass_blur.npz\n",
      " 3 : MVTec-AD_pill.npz\n",
      " 4 : MNIST-C_rotate.npz\n",
      " 5 : MVTec-AD_screw.npz\n",
      " 6 : SVHN_8.npz\n",
      " 7 : SVHN_9.npz\n",
      " 8 : MNIST-C_dotted_line.npz\n",
      " 9 : MNIST-C_shear.npz\n",
      " 10 : FashionMNIST_3.npz\n",
      " 11 : MVTec-AD_toothbrush.npz\n",
      " 12 : FashionMNIST_2.npz\n",
      " 13 : FashionMNIST_0.npz\n",
      " 14 : FashionMNIST_1.npz\n",
      " 15 : FashionMNIST_5.npz\n",
      " 16 : MNIST-C_identity.npz\n",
      " 17 : MVTec-AD_leather.npz\n",
      " 18 : FashionMNIST_4.npz\n",
      " 19 : FashionMNIST_6.npz\n",
      " 20 : CIFAR10_8.npz\n",
      " 21 : CIFAR10_9.npz\n",
      " 22 : MVTec-AD_metal_nut.npz\n",
      " 23 : FashionMNIST_7.npz\n",
      " 24 : MNIST-C_motion_blur.npz\n",
      " 25 : MNIST-C_stripe.npz\n",
      " 26 : MNIST-C_impulse_noise.npz\n",
      " 27 : MNIST-C_translate.npz\n",
      " 28 : CIFAR10_4.npz\n",
      " 29 : CIFAR10_5.npz\n",
      " 30 : FashionMNIST_9.npz\n",
      " 31 : MNIST-C_brightness.npz\n",
      " 32 : CIFAR10_7.npz\n",
      " 33 : CIFAR10_6.npz\n",
      " 34 : FashionMNIST_8.npz\n",
      " 35 : CIFAR10_2.npz\n",
      " 36 : MVTec-AD_tile.npz\n",
      " 37 : CIFAR10_3.npz\n",
      " 38 : MVTec-AD_carpet.npz\n",
      " 39 : CIFAR10_1.npz\n",
      " 40 : MVTec-AD_capsule.npz\n",
      " 41 : CIFAR10_0.npz\n",
      " 42 : MNIST-C_canny_edges.npz\n",
      " 43 : SVHN_7.npz\n",
      " 44 : MVTec-AD_grid.npz\n",
      " 45 : MNIST-C_fog.npz\n",
      " 46 : MVTec-AD_bottle.npz\n",
      " 47 : SVHN_6.npz\n",
      " 48 : SVHN_4.npz\n",
      " 49 : MVTec-AD_wood.npz\n",
      " 50 : MVTec-AD_zipper.npz\n",
      " 51 : MVTec-AD_cable.npz\n",
      " 52 : SVHN_5.npz\n",
      " 53 : MNIST-C_zigzag.npz\n",
      " 54 : SVHN_1.npz\n",
      " 55 : MNIST-C_scale.npz\n",
      " 56 : SVHN_0.npz\n",
      " 57 : SVHN_2.npz\n",
      " 58 : MNIST-C_shot_noise.npz\n",
      " 59 : MVTec-AD_transistor.npz\n",
      " 60 : MVTec-AD_hazelnut.npz\n",
      " 61 : SVHN_3.npz\n",
      "\n",
      "Classical directory:\n",
      " 1 : 26_optdigits.npz\n",
      " 2 : 42_WBC.npz\n",
      " 3 : 21_Lymphography.npz\n",
      " 4 : 8_celeba.npz\n",
      " 5 : 33_skin.npz\n",
      " 6 : 34_smtp.npz\n",
      " 7 : 28_pendigits.npz\n",
      " 8 : 39_vertebral.npz\n",
      " 9 : 11_donors.npz\n",
      " 10 : 43_WDBC.npz\n",
      " 11 : 7_Cardiotocography.npz\n",
      " 12 : 36_speech.npz\n",
      " 13 : 5_campaign.npz\n",
      " 14 : 44_Wilt.npz\n",
      " 15 : 10_cover.npz\n",
      " 16 : 46_WPBC.npz\n",
      " 17 : 37_Stamps.npz\n",
      " 18 : 2_annthyroid.npz\n",
      " 19 : 27_PageBlocks.npz\n",
      " 20 : 31_satimage-2.npz\n",
      " 21 : 3_backdoor.npz\n",
      " 22 : 38_thyroid.npz\n",
      " 23 : 29_Pima.npz\n",
      " 24 : 24_mnist.npz\n",
      " 25 : 15_Hepatitis.npz\n",
      " 26 : 22_magic.gamma.npz\n",
      " 27 : 16_http.npz\n",
      " 28 : 32_shuttle.npz\n",
      " 29 : 12_fault.npz\n",
      " 30 : 47_yeast.npz\n",
      " 31 : 13_fraud.npz\n",
      " 32 : 35_SpamBase.npz\n",
      " 33 : 41_Waveform.npz\n",
      " 34 : 17_InternetAds.npz\n",
      " 35 : 6_cardio.npz\n",
      " 36 : 23_mammography.npz\n",
      " 37 : 40_vowels.npz\n",
      " 38 : 9_census.npz\n",
      " 39 : 45_wine.npz\n",
      " 40 : 25_musk.npz\n",
      " 41 : 1_ALOI.npz\n",
      " 42 : 18_Ionosphere.npz\n",
      " 43 : 20_letter.npz\n",
      " 44 : 19_landsat.npz\n",
      " 45 : 14_glass.npz\n",
      " 46 : 30_satellite.npz\n",
      " 47 : 4_breastw.npz\n",
      "\n",
      "NLP_by_BERT directory:\n",
      " 1 : yelp.npz\n",
      " 2 : imdb.npz\n",
      " 3 : 20news_4.npz\n",
      " 4 : agnews_2.npz\n",
      " 5 : agnews_3.npz\n",
      " 6 : 20news_5.npz\n",
      " 7 : agnews_1.npz\n",
      " 8 : agnews_0.npz\n",
      " 9 : 20news_2.npz\n",
      " 10 : 20news_3.npz\n",
      " 11 : 20news_1.npz\n",
      " 12 : 20news_0.npz\n",
      " 13 : amazon.npz\n"
     ]
    }
   ],
   "source": [
    "# This code will list the file names in the directories, \n",
    "for item in os.listdir(datasets_path):\n",
    "    if not str(item).startswith('__'): \n",
    "        item_path = os.path.join(datasets_path, item)\n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(item_path) and str(item_path) != \"__pycache__\":\n",
    "            print(f\"\\n{item} directory:\")\n",
    "            \n",
    "            for ind, sub_item in enumerate(os.listdir(item_path), start=1):\n",
    "                sub_item_path = os.path.join(item_path, sub_item)\n",
    "                if os.path.isfile(sub_item_path):\n",
    "                    print(f\" {ind} : {sub_item}\")\n",
    "                elif os.path.isdir(sub_item_path):\n",
    "                    print(f\"\\t\\t{item}: {sub_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e510fd22-5a50-4462-8f23-bd6e55cf7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donors dataset: ---------------------------------: \n",
      "(619326, 10)\n",
      "(619326,)\n",
      "Cardio dataset: ---------------------------------: \n",
      "(1831, 21)\n",
      "(1831,)\n",
      "Cardio dataset: ---------------------------------: \n",
      "(284807, 29)\n",
      "(284807,)\n"
     ]
    }
   ],
   "source": [
    "# Check few datasets: Donors, Cardio, and fraud\n",
    "import numpy as np\n",
    "dataset_dir = datasets.__path__[0]\n",
    "donors_path = os.path.join(dataset_dir, \"Classical/11_donors.npz\")\n",
    "cardio_path = os.path.join(dataset_dir, \"Classical/6_cardio.npz\")\n",
    "fraud_path = os.path.join(dataset_dir, \"Classical/13_fraud.npz\")\n",
    "\n",
    "donors = np.load(donors_path, allow_pickle=True)\n",
    "cardio = np.load(cardio_path, allow_pickle=True)\n",
    "fraud = np.load(fraud_path, allow_pickle=True)\n",
    "\n",
    "print(\"Donors dataset: ---------------------------------: \")\n",
    "X, y = donors['X'], donors['y']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"Cardio dataset: ---------------------------------: \")\n",
    "X, y = cardio['X'], cardio['y']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"Cardio dataset: ---------------------------------: \")\n",
    "X, y = fraud['X'], fraud['y']\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ae69a-bd06-4efa-bed9-ae667753ab7b",
   "metadata": {},
   "source": [
    "The current directory structure for saving datasets is not optimal for this project. I prefer a more organized and accessible location where specific datasets can be easily accessed. While the existing setup may work for some, I intend to write a script to download datasets to a directory of my choice, allowing for more control over data management and accessibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69548ac-c7a6-4918-99f3-f7df4948a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/saad/Programmer/RemoteRepos/Anomaly-Detection-Resources\n",
      "All datasets are already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "from adbench.myutils import Utils\n",
    "\n",
    "# Access the project root directory from the environment variable\n",
    "# Ensure that the ANOMALY_DETECTION_PATH environment variable is set\n",
    "# if not set here like this\n",
    "# project_root = \"Your/path/to/anomaly-detection-project\"   # uncomment this before run unless\n",
    "                                                            # You set up Your project directory path\n",
    "                                                            # as an environment variable\n",
    "\n",
    "project_root = os.getenv('ANOMALY_DETECTION_PATH')\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "if project_root is None:\n",
    "    raise EnvironmentError(\"The ANOMALY_DETECTION_PATH environment variable is not set.\")\n",
    "\n",
    "# Define the dataset directory \n",
    "dataset_dir = os.path.join(project_root, 'datasets')\n",
    "\n",
    "# Ensure the dataset directory exists\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the utility class\n",
    "utils = Utils()\n",
    "\n",
    "# List of folders expected to contain datasets\n",
    "expected_folders = ['CV_by_ResNet18', 'NLP_by_BERT', 'Classical']\n",
    "\n",
    "\n",
    "# A function to check if all datasets are already downloaded\n",
    "def check_datasets_exist(base_dir, folders, files_dict=None):\n",
    "    \"\"\"Checks if all specified datasets are present in the base directory.\n",
    "\n",
    "    This function verifies the presence of specified folders within a base directory \n",
    "    and checks if the required files within those folders exist.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): The base directory where datasets are expected to be stored.\n",
    "        folders (list of str): A list of folder names expected to be present in the base directory.\n",
    "        files_dict (dict, optional): A dictionary mapping folder names to a list of expected filenames.\n",
    "                                     Default is None, meaning no specific file check is performed.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all specified folders and their required files are present, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the base directory does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        raise FileNotFoundError(f\"The base directory does not exist: {base_dir}\")\n",
    "    \n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Missing folder: {folder_path}\")\n",
    "            return False\n",
    "        \n",
    "        folder_contents = os.listdir(folder_path)\n",
    "        if not folder_contents:\n",
    "            print(f\"Folder is empty: {folder_path}\")\n",
    "            return False\n",
    "        \n",
    "        if files_dict:\n",
    "            expected_files = files_dict.get(folder, [])\n",
    "            for expected_file in expected_files:\n",
    "                file_path = os.path.join(folder_path, expected_file)\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"Missing file: {file_path}\")\n",
    "                    return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Check if all datasets are already downloaded\n",
    "if check_datasets_exist(dataset_dir, expected_folders):\n",
    "    print(\"All datasets are already downloaded.\")\n",
    "else:\n",
    "    # Attempt to download datasets\n",
    "    try:\n",
    "        utils.download_datasets(repo='jihulab')                    \n",
    "        print(\"Datasets downloaded successfully using adbench.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98de0c0-3f91-4cf0-82cc-4a1359930197",
   "metadata": {},
   "source": [
    "## Method 2: Cloning the `adbench` repository\n",
    "\n",
    "Cloning the adbench repository is the alternative approach for accessing the anomaly detection benchmark datasets. \n",
    "\n",
    "This approach allows you to obtain all datasets along the source code and supplementary materials directly from the source, offering full control over the data. \n",
    "\n",
    "> **⚠️ Note:**  \n",
    "> The size of this repository is almost three gigabytes (~3GB).\n",
    "\n",
    "Here’s how you can clone the repository and access the datasets:\n",
    "\n",
    "\n",
    "1.\t**Open a Terminal**: Begin by opening a terminal on your machine. Ensure you have `Git` or `GitHub CLI` utility installed, as it will be used to clone the repository.\n",
    "2.\t**Navigate to the Desired Directory**: Choose the directory where you created the anomaly detection project  to clone the `adbench` repository. You can navigate to this directory using the cd command:\n",
    "  ```bash\n",
    "  cd /path/to/your/project/location\n",
    "  ```\n",
    "\n",
    "3.\tClone the Repository: Use the following Git command to clone the adbench repository:\n",
    "  ```bash\n",
    "  git clone https://github.com/Minqi824/ADBench.git\n",
    "  ```\n",
    "\n",
    "or \n",
    "\n",
    "  ```bash\n",
    "  gh repo clone Minqi824/ADBench\n",
    "  ```\n",
    "\n",
    "This command will download the entire repository, including all datasets, scripts, and documentation, to your local machine.\n",
    "\n",
    "4.\tNavigate to the Cloned Repository: Once the cloning process is complete, navigate into the cloned repository:\n",
    "\n",
    "```bash\n",
    "cd ADBench\n",
    "```\n",
    "\n",
    "5.\t**Access the Datasets**: The datasets will be located in the datasets directory within the cloned repository. You can list the contents of this directory to verify the datasets:\n",
    "\n",
    "```bash\n",
    "cd adbench/datasets\n",
    "ls -la\n",
    "```\n",
    "\n",
    "This will display all the available datasets organized in their respective directories.\n",
    "\n",
    "or you can use the `tree` command:\n",
    "\n",
    "  ```sh\n",
    "  tree adbench/datasets\n",
    "  ```\n",
    "\n",
    "Here is the output of the previous command:\n",
    "\n",
    "```text\n",
    "adbench/datasets\n",
    "├── CV_by_ResNet18\n",
    "│   ├── CIFAR10_0.npz\n",
    "│   ├── CIFAR10_1.npz\n",
    "│   ├── CIFAR10_2.npz\n",
    "│   ├── CIFAR10_3.npz\n",
    "│   ├── CIFAR10_4.npz\n",
    "│   ├── CIFAR10_5.npz\n",
    "│   ├── CIFAR10_6.npz\n",
    "│   ├── CIFAR10_7.npz\n",
    "│   ├── CIFAR10_8.npz\n",
    "│   ├── CIFAR10_9.npz\n",
    "│   ├── FashionMNIST_0.npz\n",
    "│   ├── FashionMNIST_1.npz\n",
    "│   ├── FashionMNIST_2.npz\n",
    "│   ├── FashionMNIST_3.npz\n",
    "│   ├── FashionMNIST_4.npz\n",
    "│   ├── FashionMNIST_5.npz\n",
    "│   ├── FashionMNIST_6.npz\n",
    "│   ├── FashionMNIST_7.npz\n",
    "│   ├── FashionMNIST_8.npz\n",
    "│   ├── FashionMNIST_9.npz\n",
    "│   ├── MNIST-C_brightness.npz\n",
    "│   ├── MNIST-C_canny_edges.npz\n",
    "│   ├── MNIST-C_dotted_line.npz\n",
    "│   ├── MNIST-C_fog.npz\n",
    "│   ├── MNIST-C_glass_blur.npz\n",
    "│   ├── MNIST-C_identity.npz\n",
    "│   ├── MNIST-C_impulse_noise.npz\n",
    "│   ├── MNIST-C_motion_blur.npz\n",
    "│   ├── MNIST-C_rotate.npz\n",
    "│   ├── MNIST-C_scale.npz\n",
    "│   ├── MNIST-C_shear.npz\n",
    "│   ├── MNIST-C_shot_noise.npz\n",
    "│   ├── MNIST-C_spatter.npz\n",
    "│   ├── MNIST-C_stripe.npz\n",
    "│   ├── MNIST-C_translate.npz\n",
    "│   ├── MNIST-C_zigzag.npz\n",
    "│   ├── MVTec-AD_bottle.npz\n",
    "│   ├── MVTec-AD_cable.npz\n",
    "│   ├── MVTec-AD_capsule.npz\n",
    "│   ├── MVTec-AD_carpet.npz\n",
    "│   ├── MVTec-AD_grid.npz\n",
    "│   ├── MVTec-AD_hazelnut.npz\n",
    "│   ├── MVTec-AD_leather.npz\n",
    "│   ├── MVTec-AD_metal_nut.npz\n",
    "│   ├── MVTec-AD_pill.npz\n",
    "│   ├── MVTec-AD_screw.npz\n",
    "│   ├── MVTec-AD_tile.npz\n",
    "│   ├── MVTec-AD_toothbrush.npz\n",
    "│   ├── MVTec-AD_transistor.npz\n",
    "│   ├── MVTec-AD_wood.npz\n",
    "│   ├── MVTec-AD_zipper.npz\n",
    "│   ├── SVHN_0.npz\n",
    "│   ├── SVHN_1.npz\n",
    "│   ├── SVHN_2.npz\n",
    "│   ├── SVHN_3.npz\n",
    "│   ├── SVHN_4.npz\n",
    "│   ├── SVHN_5.npz\n",
    "│   ├── SVHN_6.npz\n",
    "│   ├── SVHN_7.npz\n",
    "│   ├── SVHN_8.npz\n",
    "│   └── SVHN_9.npz\n",
    "├── CV_by_ViT\n",
    "│   ├── CIFAR10_0.npz\n",
    "│   ├── CIFAR10_1.npz\n",
    "│   ├── CIFAR10_2.npz\n",
    "│   ├── CIFAR10_3.npz\n",
    "│   ├── CIFAR10_4.npz\n",
    "│   ├── CIFAR10_5.npz\n",
    "│   ├── CIFAR10_6.npz\n",
    "│   ├── CIFAR10_7.npz\n",
    "│   ├── CIFAR10_8.npz\n",
    "│   ├── CIFAR10_9.npz\n",
    "│   ├── FashionMNIST_0.npz\n",
    "│   ├── FashionMNIST_1.npz\n",
    "│   ├── FashionMNIST_2.npz\n",
    "│   ├── FashionMNIST_3.npz\n",
    "│   ├── FashionMNIST_4.npz\n",
    "│   ├── FashionMNIST_5.npz\n",
    "│   ├── FashionMNIST_6.npz\n",
    "│   ├── FashionMNIST_7.npz\n",
    "│   ├── FashionMNIST_8.npz\n",
    "│   ├── FashionMNIST_9.npz\n",
    "│   ├── MNIST-C_brightness.npz\n",
    "│   ├── MNIST-C_canny_edges.npz\n",
    "│   ├── MNIST-C_dotted_line.npz\n",
    "│   ├── MNIST-C_fog.npz\n",
    "│   ├── MNIST-C_glass_blur.npz\n",
    "│   ├── MNIST-C_identity.npz\n",
    "│   ├── MNIST-C_impulse_noise.npz\n",
    "│   ├── MNIST-C_motion_blur.npz\n",
    "│   ├── MNIST-C_rotate.npz\n",
    "│   ├── MNIST-C_scale.npz\n",
    "│   ├── MNIST-C_shear.npz\n",
    "│   ├── MNIST-C_shot_noise.npz\n",
    "│   ├── MNIST-C_spatter.npz\n",
    "│   ├── MNIST-C_stripe.npz\n",
    "│   ├── MNIST-C_translate.npz\n",
    "│   ├── MNIST-C_zigzag.npz\n",
    "│   ├── MVTec-AD_bottle.npz\n",
    "│   ├── MVTec-AD_cable.npz\n",
    "│   ├── MVTec-AD_capsule.npz\n",
    "│   ├── MVTec-AD_carpet.npz\n",
    "│   ├── MVTec-AD_grid.npz\n",
    "│   ├── MVTec-AD_hazelnut.npz\n",
    "│   ├── MVTec-AD_leather.npz\n",
    "│   ├── MVTec-AD_metal_nut.npz\n",
    "│   ├── MVTec-AD_pill.npz\n",
    "│   ├── MVTec-AD_screw.npz\n",
    "│   ├── MVTec-AD_tile.npz\n",
    "│   ├── MVTec-AD_toothbrush.npz\n",
    "│   ├── MVTec-AD_transistor.npz\n",
    "│   ├── MVTec-AD_wood.npz\n",
    "│   ├── MVTec-AD_zipper.npz\n",
    "│   ├── SVHN_0.npz\n",
    "│   ├── SVHN_1.npz\n",
    "│   ├── SVHN_2.npz\n",
    "│   ├── SVHN_3.npz\n",
    "│   ├── SVHN_4.npz\n",
    "│   ├── SVHN_5.npz\n",
    "│   ├── SVHN_6.npz\n",
    "│   ├── SVHN_7.npz\n",
    "│   ├── SVHN_8.npz\n",
    "│   └── SVHN_9.npz\n",
    "├── Classical\n",
    "│   ├── 10_cover.npz\n",
    "│   ├── 11_donors.npz\n",
    "│   ├── 12_fault.npz\n",
    "│   ├── 13_fraud.npz\n",
    "│   ├── 14_glass.npz\n",
    "│   ├── 15_Hepatitis.npz\n",
    "│   ├── 16_http.npz\n",
    "│   ├── 17_InternetAds.npz\n",
    "│   ├── 18_Ionosphere.npz\n",
    "│   ├── 19_landsat.npz\n",
    "│   ├── 1_ALOI.npz\n",
    "│   ├── 20_letter.npz\n",
    "│   ├── 21_Lymphography.npz\n",
    "│   ├── 22_magic.gamma.npz\n",
    "│   ├── 23_mammography.npz\n",
    "│   ├── 24_mnist.npz\n",
    "│   ├── 25_musk.npz\n",
    "│   ├── 26_optdigits.npz\n",
    "│   ├── 27_PageBlocks.npz\n",
    "│   ├── 28_pendigits.npz\n",
    "│   ├── 29_Pima.npz\n",
    "│   ├── 2_annthyroid.npz\n",
    "│   ├── 30_satellite.npz\n",
    "│   ├── 31_satimage-2.npz\n",
    "│   ├── 32_shuttle.npz\n",
    "│   ├── 33_skin.npz\n",
    "│   ├── 34_smtp.npz\n",
    "│   ├── 35_SpamBase.npz\n",
    "│   ├── 36_speech.npz\n",
    "│   ├── 37_Stamps.npz\n",
    "│   ├── 38_thyroid.npz\n",
    "│   ├── 39_vertebral.npz\n",
    "│   ├── 3_backdoor.npz\n",
    "│   ├── 40_vowels.npz\n",
    "│   ├── 41_Waveform.npz\n",
    "│   ├── 42_WBC.npz\n",
    "│   ├── 43_WDBC.npz\n",
    "│   ├── 44_Wilt.npz\n",
    "│   ├── 45_wine.npz\n",
    "│   ├── 46_WPBC.npz\n",
    "│   ├── 47_yeast.npz\n",
    "│   ├── 4_breastw.npz\n",
    "│   ├── 5_campaign.npz\n",
    "│   ├── 6_cardio.npz\n",
    "│   ├── 7_Cardiotocography.npz\n",
    "│   ├── 8_celeba.npz\n",
    "│   └── 9_census.npz\n",
    "├── NLP_by_BERT\n",
    "│   ├── 20news_0.npz\n",
    "│   ├── 20news_1.npz\n",
    "│   ├── 20news_2.npz\n",
    "│   ├── 20news_3.npz\n",
    "│   ├── 20news_4.npz\n",
    "│   ├── 20news_5.npz\n",
    "│   ├── agnews_0.npz\n",
    "│   ├── agnews_1.npz\n",
    "│   ├── agnews_2.npz\n",
    "│   ├── agnews_3.npz\n",
    "│   ├── amazon.npz\n",
    "│   ├── imdb.npz\n",
    "│   └── yelp.npz\n",
    "├── NLP_by_RoBERTa\n",
    "│   ├── 20news_0.npz\n",
    "│   ├── 20news_1.npz\n",
    "│   ├── 20news_2.npz\n",
    "│   ├── 20news_3.npz\n",
    "│   ├── 20news_4.npz\n",
    "│   ├── 20news_5.npz\n",
    "│   ├── agnews_0.npz\n",
    "│   ├── agnews_1.npz\n",
    "│   ├── agnews_2.npz\n",
    "│   ├── agnews_3.npz\n",
    "│   ├── amazon.npz\n",
    "│   ├── imdb.npz\n",
    "│   └── yelp.npz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e247bda-83d5-44af-928f-5dc23f402b85",
   "metadata": {},
   "source": [
    "Now we can access any dataset from the previous list. For example, we can load and check the shape of the:\n",
    "  1. amazon\n",
    "  2. yelp\n",
    "\n",
    "from `NLP_by_RoBERTa` directory like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e04df9-cb67-4b27-acee-849b9ca2d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_file = \"../../ADBench/adbench/datasets/NLP_by_RoBERTa/amazon.npz\" \n",
    "census_file = \"../../ADBench/adbench/datasets/Classical/9_census.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "680ca595-9611-45d2-9228-e5b081197907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon dataset: ---------------------------------: \n",
      "(10000, 768)\n",
      "(10000,)\n",
      "Census dataset: ---------------------------------: \n",
      "(299285, 500)\n",
      "(299285,)\n"
     ]
    }
   ],
   "source": [
    "amazon_data = np.load(amazon_file, allow_pickle=True)\n",
    "census_data = np.load(census_file, allow_pickle=True)\n",
    "print(\"Amazon dataset: ---------------------------------: \")\n",
    "X, y = amazon_data['X'], amazon_data['y']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"Census dataset: ---------------------------------: \")\n",
    "X, y = census_data['X'], census_data['y']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bb2d9-d4aa-4367-82c3-71c750361859",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates multiple methods for downloading anomaly detection datasets:\n",
    "  1. Using the utility function provided by the authors of `adbench`\n",
    "  2. A custom way to download the datasets into a desired location.\n",
    "  3. Exploring few datasets\n",
    "  4. Cloning the entire `ADBench` repository.\n",
    "\n",
    "If you found this notebook helpful, please consider giving us a rating, starring this repository, and supporting our work in any way you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21857b97-80b0-453e-9fbf-ff5a6dc0a01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly-Detection",
   "language": "python",
   "name": "anom-detect-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
