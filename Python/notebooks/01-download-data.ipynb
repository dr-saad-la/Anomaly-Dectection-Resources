{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c05db5-b40c-4367-81f7-298250d52729",
   "metadata": {},
   "source": [
    "# Anomaly Detection Benchmark Data\n",
    "\n",
    "In this notebook, we will demonstrate how to download anomaly detection benchmark datasets. These datasets are used for evaluating and testing various **anomaly detection algorithms**.\n",
    "\n",
    "We will explore two methods to obtain the datasets:\n",
    "\n",
    "1. Using the `adbench` library to download data.\n",
    "2. Cloning the repository directly.\n",
    "\n",
    "\n",
    "## Method 1: Using the `adbench` Library\n",
    "\n",
    "1. **Check installation**: Make sure the `adbench` library is installed. If it's not installed, please refer to [anomaly detectin environment setup](00-setup.ipynb) for detailed installation instructions.\n",
    "2. **Download Datasets Using adbench**: Use the following Python script to download the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0c7f36-4cf3-47eb-9be4-43c1edbafe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if there is any question while downloading datasets, we suggest you to download it from the website:\n",
      "https://github.com/Minqi824/ADBench/tree/main/adbench/datasets\n",
      "如果您在中国大陆地区，请使用链接：\n",
      "https://jihulab.com/BraudoCC/ADBench_datasets/\n",
      "Downloading datasets from jihulab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 805.77it/s]\n",
      "INFO:__main__:Datasets downloaded successfully using adbench.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10_0.npz already exists. Skipping download...\n",
      "CIFAR10_1.npz already exists. Skipping download...\n",
      "CIFAR10_2.npz already exists. Skipping download...\n",
      "CIFAR10_3.npz already exists. Skipping download...\n",
      "CIFAR10_4.npz already exists. Skipping download...\n",
      "CIFAR10_5.npz already exists. Skipping download...\n",
      "CIFAR10_6.npz already exists. Skipping download...\n",
      "CIFAR10_7.npz already exists. Skipping download...\n",
      "CIFAR10_8.npz already exists. Skipping download...\n",
      "CIFAR10_9.npz already exists. Skipping download...\n",
      "FashionMNIST_0.npz already exists. Skipping download...\n",
      "FashionMNIST_1.npz already exists. Skipping download...\n",
      "FashionMNIST_2.npz already exists. Skipping download...\n",
      "FashionMNIST_3.npz already exists. Skipping download...\n",
      "FashionMNIST_4.npz already exists. Skipping download...\n",
      "FashionMNIST_5.npz already exists. Skipping download...\n",
      "FashionMNIST_6.npz already exists. Skipping download...\n",
      "FashionMNIST_7.npz already exists. Skipping download...\n",
      "FashionMNIST_8.npz already exists. Skipping download...\n",
      "FashionMNIST_9.npz already exists. Skipping download...\n",
      "MNIST-C_brightness.npz already exists. Skipping download...\n",
      "MNIST-C_canny_edges.npz already exists. Skipping download...\n",
      "MNIST-C_dotted_line.npz already exists. Skipping download...\n",
      "MNIST-C_fog.npz already exists. Skipping download...\n",
      "MNIST-C_glass_blur.npz already exists. Skipping download...\n",
      "MNIST-C_identity.npz already exists. Skipping download...\n",
      "MNIST-C_impulse_noise.npz already exists. Skipping download...\n",
      "MNIST-C_motion_blur.npz already exists. Skipping download...\n",
      "MNIST-C_rotate.npz already exists. Skipping download...\n",
      "MNIST-C_scale.npz already exists. Skipping download...\n",
      "MNIST-C_shear.npz already exists. Skipping download...\n",
      "MNIST-C_shot_noise.npz already exists. Skipping download...\n",
      "MNIST-C_spatter.npz already exists. Skipping download...\n",
      "MNIST-C_stripe.npz already exists. Skipping download...\n",
      "MNIST-C_translate.npz already exists. Skipping download...\n",
      "MNIST-C_zigzag.npz already exists. Skipping download...\n",
      "MVTec-AD_bottle.npz already exists. Skipping download...\n",
      "MVTec-AD_cable.npz already exists. Skipping download...\n",
      "MVTec-AD_capsule.npz already exists. Skipping download...\n",
      "MVTec-AD_carpet.npz already exists. Skipping download...\n",
      "MVTec-AD_grid.npz already exists. Skipping download...\n",
      "MVTec-AD_hazelnut.npz already exists. Skipping download...\n",
      "MVTec-AD_leather.npz already exists. Skipping download...\n",
      "MVTec-AD_metal_nut.npz already exists. Skipping download...\n",
      "MVTec-AD_pill.npz already exists. Skipping download...\n",
      "MVTec-AD_screw.npz already exists. Skipping download...\n",
      "MVTec-AD_tile.npz already exists. Skipping download...\n",
      "MVTec-AD_toothbrush.npz already exists. Skipping download...\n",
      "MVTec-AD_transistor.npz already exists. Skipping download...\n",
      "MVTec-AD_wood.npz already exists. Skipping download...\n",
      "MVTec-AD_zipper.npz already exists. Skipping download...\n",
      "SVHN_0.npz already exists. Skipping download...\n",
      "SVHN_1.npz already exists. Skipping download...\n",
      "SVHN_2.npz already exists. Skipping download...\n",
      "SVHN_3.npz already exists. Skipping download...\n",
      "SVHN_4.npz already exists. Skipping download...\n",
      "SVHN_5.npz already exists. Skipping download...\n",
      "SVHN_6.npz already exists. Skipping download...\n",
      "SVHN_7.npz already exists. Skipping download...\n",
      "SVHN_8.npz already exists. Skipping download...\n",
      "SVHN_9.npz already exists. Skipping download...\n",
      "20news_0.npz already exists. Skipping download...\n",
      "20news_1.npz already exists. Skipping download...\n",
      "20news_2.npz already exists. Skipping download...\n",
      "20news_3.npz already exists. Skipping download...\n",
      "20news_4.npz already exists. Skipping download...\n",
      "20news_5.npz already exists. Skipping download...\n",
      "agnews_0.npz already exists. Skipping download...\n",
      "agnews_1.npz already exists. Skipping download...\n",
      "agnews_2.npz already exists. Skipping download...\n",
      "agnews_3.npz already exists. Skipping download...\n",
      "amazon.npz already exists. Skipping download...\n",
      "imdb.npz already exists. Skipping download...\n",
      "yelp.npz already exists. Skipping download...\n",
      "10_cover.npz already exists. Skipping download...\n",
      "11_donors.npz already exists. Skipping download...\n",
      "12_fault.npz already exists. Skipping download...\n",
      "13_fraud.npz already exists. Skipping download...\n",
      "14_glass.npz already exists. Skipping download...\n",
      "15_Hepatitis.npz already exists. Skipping download...\n",
      "16_http.npz already exists. Skipping download...\n",
      "17_InternetAds.npz already exists. Skipping download...\n",
      "18_Ionosphere.npz already exists. Skipping download...\n",
      "19_landsat.npz already exists. Skipping download...\n",
      "1_ALOI.npz already exists. Skipping download...\n",
      "20_letter.npz already exists. Skipping download...\n",
      "21_Lymphography.npz already exists. Skipping download...\n",
      "22_magic.gamma.npz already exists. Skipping download...\n",
      "23_mammography.npz already exists. Skipping download...\n",
      "24_mnist.npz already exists. Skipping download...\n",
      "25_musk.npz already exists. Skipping download...\n",
      "26_optdigits.npz already exists. Skipping download...\n",
      "27_PageBlocks.npz already exists. Skipping download...\n",
      "28_pendigits.npz already exists. Skipping download...\n",
      "29_Pima.npz already exists. Skipping download...\n",
      "2_annthyroid.npz already exists. Skipping download...\n",
      "30_satellite.npz already exists. Skipping download...\n",
      "31_satimage-2.npz already exists. Skipping download...\n",
      "32_shuttle.npz already exists. Skipping download...\n",
      "33_skin.npz already exists. Skipping download...\n",
      "34_smtp.npz already exists. Skipping download...\n",
      "35_SpamBase.npz already exists. Skipping download...\n",
      "36_speech.npz already exists. Skipping download...\n",
      "37_Stamps.npz already exists. Skipping download...\n",
      "38_thyroid.npz already exists. Skipping download...\n",
      "39_vertebral.npz already exists. Skipping download...\n",
      "3_backdoor.npz already exists. Skipping download...\n",
      "40_vowels.npz already exists. Skipping download...\n",
      "41_Waveform.npz already exists. Skipping download...\n",
      "42_WBC.npz already exists. Skipping download...\n",
      "43_WDBC.npz already exists. Skipping download...\n",
      "44_Wilt.npz already exists. Skipping download...\n",
      "45_wine.npz already exists. Skipping download...\n",
      "46_WPBC.npz already exists. Skipping download...\n",
      "47_yeast.npz already exists. Skipping download...\n",
      "4_breastw.npz already exists. Skipping download...\n",
      "5_campaign.npz already exists. Skipping download...\n",
      "6_cardio.npz already exists. Skipping download...\n",
      "7_Cardiotocography.npz already exists. Skipping download...\n",
      "8_celeba.npz already exists. Skipping download...\n",
      "9_census.npz already exists. Skipping download...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from adbench.myutils import Utils\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the utility class\n",
    "utils = Utils()\n",
    "\n",
    "# Attempt to download datasets with error handling\n",
    "try:\n",
    "    # Download datasets from the specified repository\n",
    "    utils.download_datasets(repo='jihulab')                          # Use 'jihulab' if you're in China\n",
    "    logger.info(\"Datasets downloaded successfully using adbench.\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        logger.info(\"Datasets are already downloaded.\")\n",
    "    else:\n",
    "        # Log unexpected exceptions\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27cc2e-f6be-456a-88c6-eaf83e73f42b",
   "metadata": {},
   "source": [
    "After running the previous script, I had to go through the source code of `download_datasets()` method to figure out the directory where the data was stored. \n",
    "\n",
    "I also did the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf0666c-0c3e-42f9-9ee6-6dc0d7e5f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "# 1. import datasets from adbench\n",
    "from adbench import datasets\n",
    "\n",
    "# 2. Check attributes of this module\n",
    "print(dir(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d5b5fa0-8d34-4f05-bc51-780b0669f0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/homebrew/Caskroom/mambaforge/base/envs/anom-detect-env/lib/python3.9/site-packages/adbench/datasets']\n",
      "['CV_by_ResNet18', '__init__.py', '__pycache__', 'Classical', 'data_generator.py', 'NLP_by_BERT']\n"
     ]
    }
   ],
   "source": [
    "# 3. Cheching the __path__ attribute\n",
    "print(datasets.__path__)\n",
    "\n",
    "# 4. List the content of directory\n",
    "import os\n",
    "print(os.listdir(datasets.__path__[0]))\n",
    "\n",
    "# Path to the datasets directory\n",
    "datasets_path = datasets.__path__[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0a2384a-862e-419e-bbc4-2739335d845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV_by_ResNet18 directory:\n",
      " 1 : MNIST-C_spatter.npz\n",
      " 2 : MNIST-C_glass_blur.npz\n",
      " 3 : MVTec-AD_pill.npz\n",
      " 4 : MNIST-C_rotate.npz\n",
      " 5 : MVTec-AD_screw.npz\n",
      " 6 : SVHN_8.npz\n",
      " 7 : SVHN_9.npz\n",
      " 8 : MNIST-C_dotted_line.npz\n",
      " 9 : MNIST-C_shear.npz\n",
      " 10 : FashionMNIST_3.npz\n",
      " 11 : MVTec-AD_toothbrush.npz\n",
      " 12 : FashionMNIST_2.npz\n",
      " 13 : FashionMNIST_0.npz\n",
      " 14 : FashionMNIST_1.npz\n",
      " 15 : FashionMNIST_5.npz\n",
      " 16 : MNIST-C_identity.npz\n",
      " 17 : MVTec-AD_leather.npz\n",
      " 18 : FashionMNIST_4.npz\n",
      " 19 : FashionMNIST_6.npz\n",
      " 20 : CIFAR10_8.npz\n",
      " 21 : CIFAR10_9.npz\n",
      " 22 : MVTec-AD_metal_nut.npz\n",
      " 23 : FashionMNIST_7.npz\n",
      " 24 : MNIST-C_motion_blur.npz\n",
      " 25 : MNIST-C_stripe.npz\n",
      " 26 : MNIST-C_impulse_noise.npz\n",
      " 27 : MNIST-C_translate.npz\n",
      " 28 : CIFAR10_4.npz\n",
      " 29 : CIFAR10_5.npz\n",
      " 30 : FashionMNIST_9.npz\n",
      " 31 : MNIST-C_brightness.npz\n",
      " 32 : CIFAR10_7.npz\n",
      " 33 : CIFAR10_6.npz\n",
      " 34 : FashionMNIST_8.npz\n",
      " 35 : CIFAR10_2.npz\n",
      " 36 : MVTec-AD_tile.npz\n",
      " 37 : CIFAR10_3.npz\n",
      " 38 : MVTec-AD_carpet.npz\n",
      " 39 : CIFAR10_1.npz\n",
      " 40 : MVTec-AD_capsule.npz\n",
      " 41 : CIFAR10_0.npz\n",
      " 42 : MNIST-C_canny_edges.npz\n",
      " 43 : SVHN_7.npz\n",
      " 44 : MVTec-AD_grid.npz\n",
      " 45 : MNIST-C_fog.npz\n",
      " 46 : MVTec-AD_bottle.npz\n",
      " 47 : SVHN_6.npz\n",
      " 48 : SVHN_4.npz\n",
      " 49 : MVTec-AD_wood.npz\n",
      " 50 : MVTec-AD_zipper.npz\n",
      " 51 : MVTec-AD_cable.npz\n",
      " 52 : SVHN_5.npz\n",
      " 53 : MNIST-C_zigzag.npz\n",
      " 54 : SVHN_1.npz\n",
      " 55 : MNIST-C_scale.npz\n",
      " 56 : SVHN_0.npz\n",
      " 57 : SVHN_2.npz\n",
      " 58 : MNIST-C_shot_noise.npz\n",
      " 59 : MVTec-AD_transistor.npz\n",
      " 60 : MVTec-AD_hazelnut.npz\n",
      " 61 : SVHN_3.npz\n",
      "\n",
      "Classical directory:\n",
      " 1 : 26_optdigits.npz\n",
      " 2 : 42_WBC.npz\n",
      " 3 : 21_Lymphography.npz\n",
      " 4 : 8_celeba.npz\n",
      " 5 : 33_skin.npz\n",
      " 6 : 34_smtp.npz\n",
      " 7 : 28_pendigits.npz\n",
      " 8 : 39_vertebral.npz\n",
      " 9 : 11_donors.npz\n",
      " 10 : 43_WDBC.npz\n",
      " 11 : 7_Cardiotocography.npz\n",
      " 12 : 36_speech.npz\n",
      " 13 : 5_campaign.npz\n",
      " 14 : 44_Wilt.npz\n",
      " 15 : 10_cover.npz\n",
      " 16 : 46_WPBC.npz\n",
      " 17 : 37_Stamps.npz\n",
      " 18 : 2_annthyroid.npz\n",
      " 19 : 27_PageBlocks.npz\n",
      " 20 : 31_satimage-2.npz\n",
      " 21 : 3_backdoor.npz\n",
      " 22 : 38_thyroid.npz\n",
      " 23 : 29_Pima.npz\n",
      " 24 : 24_mnist.npz\n",
      " 25 : 15_Hepatitis.npz\n",
      " 26 : 22_magic.gamma.npz\n",
      " 27 : 16_http.npz\n",
      " 28 : 32_shuttle.npz\n",
      " 29 : 12_fault.npz\n",
      " 30 : 47_yeast.npz\n",
      " 31 : 13_fraud.npz\n",
      " 32 : 35_SpamBase.npz\n",
      " 33 : 41_Waveform.npz\n",
      " 34 : 17_InternetAds.npz\n",
      " 35 : 6_cardio.npz\n",
      " 36 : 23_mammography.npz\n",
      " 37 : 40_vowels.npz\n",
      " 38 : 9_census.npz\n",
      " 39 : 45_wine.npz\n",
      " 40 : 25_musk.npz\n",
      " 41 : 1_ALOI.npz\n",
      " 42 : 18_Ionosphere.npz\n",
      " 43 : 20_letter.npz\n",
      " 44 : 19_landsat.npz\n",
      " 45 : 14_glass.npz\n",
      " 46 : 30_satellite.npz\n",
      " 47 : 4_breastw.npz\n",
      "\n",
      "NLP_by_BERT directory:\n",
      " 1 : yelp.npz\n",
      " 2 : imdb.npz\n",
      " 3 : 20news_4.npz\n",
      " 4 : agnews_2.npz\n",
      " 5 : agnews_3.npz\n",
      " 6 : 20news_5.npz\n",
      " 7 : agnews_1.npz\n",
      " 8 : agnews_0.npz\n",
      " 9 : 20news_2.npz\n",
      " 10 : 20news_3.npz\n",
      " 11 : 20news_1.npz\n",
      " 12 : 20news_0.npz\n",
      " 13 : amazon.npz\n"
     ]
    }
   ],
   "source": [
    "# This code will list the file names in the directories, \n",
    "for item in os.listdir(datasets_path):\n",
    "    if not str(item).startswith('__'): \n",
    "        item_path = os.path.join(datasets_path, item)\n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(item_path) and str(item_path) != \"__pycache__\":\n",
    "            print(f\"\\n{item} directory:\")\n",
    "            \n",
    "            for ind, sub_item in enumerate(os.listdir(item_path), start=1):\n",
    "                sub_item_path = os.path.join(item_path, sub_item)\n",
    "                if os.path.isfile(sub_item_path):\n",
    "                    print(f\" {ind} : {sub_item}\")\n",
    "                elif os.path.isdir(sub_item_path):\n",
    "                    print(f\"\\t\\t{item}: {sub_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e510fd22-5a50-4462-8f23-bd6e55cf7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donors dataset: ---------------------------------: \n",
      "(619326, 10)\n",
      "(619326,)\n",
      "Cardio dataset: ---------------------------------: \n",
      "(1831, 21)\n",
      "(1831,)\n",
      "Cardio dataset: ---------------------------------: \n",
      "(284807, 29)\n",
      "(284807,)\n"
     ]
    }
   ],
   "source": [
    "# Check few datasets: Donors, Cardio, and fraud\n",
    "import numpy as np\n",
    "dataset_dir = datasets.__path__[0]\n",
    "donors_path = os.path.join(dataset_dir, \"Classical/11_donors.npz\")\n",
    "cardio_path = os.path.join(dataset_dir, \"Classical/6_cardio.npz\")\n",
    "fraud_path = os.path.join(dataset_dir, \"Classical/13_fraud.npz\")\n",
    "\n",
    "donors = np.load(donors_path, allow_pickle=True)\n",
    "cardio = np.load(cardio_path, allow_pickle=True)\n",
    "fraud = np.load(fraud_path, allow_pickle=True)\n",
    "\n",
    "print(\"Donors dataset: ---------------------------------: \")\n",
    "X, y = donors['X'], donors['y']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"Cardio dataset: ---------------------------------: \")\n",
    "X, y = cardio['X'], cardio['y']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"Cardio dataset: ---------------------------------: \")\n",
    "X, y = fraud['X'], fraud['y']\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ae69a-bd06-4efa-bed9-ae667753ab7b",
   "metadata": {},
   "source": [
    "The current directory structure for saving datasets is not optimal for this project. I prefer a more organized and accessible location where specific datasets can be easily accessed. While the existing setup may work for some, I intend to write a script to download datasets to a directory of my choice, allowing for more control over data management and accessibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69548ac-c7a6-4918-99f3-f7df4948a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "from adbench.myutils import Utils\n",
    "\n",
    "# Access the project root directory from the environment variable\n",
    "# Ensure that the ANOMALY_DETECTION_PATH environment variable is set\n",
    "# if not set here like this\n",
    "# project_root = \"Your/path/to/anomaly-detection-project\"   # uncomment this before run unless\n",
    "                                                            # You set up Your project directory path\n",
    "                                                            # as an environment variable\n",
    "\n",
    "project_root = os.getenv('ANOMALY_DETECTION_PATH')\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "if project_root is None:\n",
    "    raise EnvironmentError(\"The ANOMALY_DETECTION_PATH environment variable is not set.\")\n",
    "\n",
    "# Define the dataset directory \n",
    "dataset_dir = os.path.join(project_root, 'datasets')\n",
    "print(dataset_dir)\n",
    "print(os.listdir(dataset_dir))\n",
    "\n",
    "\n",
    "# Ensure the dataset directory exists\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the utility class\n",
    "utils = Utils()\n",
    "\n",
    "# List of folders expected to contain datasets\n",
    "expected_folders = ['CV_by_ResNet18', 'NLP_by_BERT', 'Classical']\n",
    "\n",
    "\n",
    "# Define the function to check if all datasets are already downloaded\n",
    "def all_datasets_exist(base_dir, folders):\n",
    "    \"\"\"\n",
    "    Checks if all datasets are present in the specified base directory.\n",
    "\n",
    "    :param base_dir: The base directory where datasets are stored.\n",
    "    :param folders: A list of folder names expected to be present in the base directory.\n",
    "    :param files_dict: Optional dictionary specifying expected files in each folder.\n",
    "    :return: True if all folders and files are present, False otherwise.\n",
    "    \"\"\"\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        print(folder_path)\n",
    "        # Check if folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Missing folder: {folder_path}\")\n",
    "            return False\n",
    "        # Check if folder is not empty\n",
    "        if not os.listdir(folder_path):\n",
    "            print(f\"Folder is empty: {folder_path}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Check if all datasets are already downloaded\n",
    "if all_datasets_exist(dataset_dir, expected_folders):\n",
    "    print(\"All datasets are already downloaded.\")\n",
    "else:\n",
    "    # Attempt to download datasets\n",
    "    try:\n",
    "        utils.download_datasets(repo='jihulab')                    # Use 'github' if not in China\n",
    "        print(\"Datasets downloaded successfully using adbench.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during download: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98de0c0-3f91-4cf0-82cc-4a1359930197",
   "metadata": {},
   "source": [
    "## Method 2: Cloning the `adbench` repository\n",
    "\n",
    "Cloning the adbench repository is the alternative approach for accessing the anomaly detection benchmark datasets. This approach is easier and allows you to obtain all datasets and supplementary materials directly from the source, offering flexibility and full control over the data. Here’s how you can clone the repository and access the datasets:\n",
    "\n",
    "Steps to Clone the Repository\n",
    "\n",
    "1.\t**Open a Terminal**: Begin by opening a terminal on your machine. Ensure you have Git or `GitHub CLI` utility installed, as it will be used to clone the repository.\n",
    "2.\t**Navigate to the Desired Directory**: Choose the directory where you created the anomaly detection project  to clone the `adbench` repository. You can navigate to this directory using the cd command:\n",
    "```bash\n",
    "cd /path/to/your/project/location\n",
    "```\n",
    "\n",
    "3.\tClone the Repository: Use the following Git command to clone the adbench repository:\n",
    "```bash\n",
    "git clone https://github.com/Minqi824/ADBench.git\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```bash\n",
    "gh repo clone Minqi824/ADBench\n",
    "```\n",
    "\n",
    "This command will download the entire repository, including all datasets, scripts, and documentation, to your local machine.\n",
    "\n",
    "4.\tNavigate to the Cloned Repository: Once the cloning process is complete, navigate into the cloned repository:\n",
    "\n",
    "```bash\n",
    "cd ADBench\n",
    "```\n",
    "\n",
    "5.\tAccess the Datasets: The datasets will be located in the datasets directory within the cloned repository. You can list the contents of this directory to verify the datasets:\n",
    "\n",
    "```bash\n",
    "cd adbench/datasets\n",
    "ls -la\n",
    "```\n",
    "\n",
    "This will display all the available datasets organized in their respective directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a4068-db58-4031-8842-d4c676b0b9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly-Detection",
   "language": "python",
   "name": "anom-detect-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
